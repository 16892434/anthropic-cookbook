{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "813NspGRKhc2"
      },
      "source": [
        "# RAG Pipeline with LlamaIndex\n",
        "\n",
        "In this notebook we will look into building Basic RAG Pipeline with LlamaIndex. The pipeline has following steps.\n",
        "\n",
        "1. Setup LLM and Embedding Model.\n",
        "2. Download Data.\n",
        "3. Load Data.\n",
        "4. Index Data.\n",
        "5. Create Query Engine.\n",
        "6. Querying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYHCYDecKDRZ"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLTzvn__ldjd"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-llms-anthropic\n",
        "!pip install llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu-wU44BKF9c"
      },
      "source": [
        "### Setup API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku5rkxtIlpCs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['ANTHROPIC_API_KEY'] = 'YOUR ANTHROPIC API KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJTRVhkJKH4r"
      },
      "source": [
        "### Setup LLM and Embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ0tqJL_mSe0"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.anthropic import Anthropic\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snc2jpj4nlXJ"
      },
      "outputs": [],
      "source": [
        "llm = Anthropic(temperature=0.0, model='claude-2.1')\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmN2FEQFnx6Y"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqdSyD2_KK-m"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC7CD222n72t",
        "outputId": "42c54b8a-bec9-4c2e-9cd5-97387f7011eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-29 12:17:29--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2024-02-29 12:17:29 (16.2 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea9GbN2poO3V"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dOGl_SKKUNH"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4hWhnhxoUBj"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM3-kLhdKRk1"
      },
      "source": [
        "### Index Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvblCnWYKSrh"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a17Tz644KZ_P"
      },
      "source": [
        "### Create Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIT8kqYKoaRq"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6ODGRTxKd-u"
      },
      "source": [
        "### Test Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igdrBclbKYrJ"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What did author do growing up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap-6RUvSozgE",
        "outputId": "37ba0327-afe7-4f1e-d862-0412085954c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The author worked on writing short stories and programming in his youth, including experimenting with an early version of Fortran on a computer at his school. He found programming puzzling at first since he didn't have much data to input or math knowledge to draw from. Overall, the passage focuses more on his later essays and projects rather than providing full details on his childhood activities. Please let me know if you need any clarification or have additional questions!\n"
          ]
        }
      ],
      "source": [
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri47de5wpMTo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3 (main, Apr  7 2023, 19:08:44) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
